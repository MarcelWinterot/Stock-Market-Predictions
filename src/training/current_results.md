# Results from experiments I've performed

## Model architectures I've tested

### 1. Plain transformers

#### Simmilar peroformance and higher training time compared to simple RNNs with attention

### 2. Time2Vec

#### Worse results than without it, might be due to incorrect use on my part

### 3. LSTM with attention

#### Simillar performance to transoformers but faster training time

### 4. Stacked RNNs

#### Architecutre simillar to N-Beats, perform a lot better than any previous architecture I've tested

## Model architectures yet to test

### 1. Custom transformers for time series predictions

#### It can be all kind of transformers like PatchTST or an original design that prioritizes later inputs

### 2. HMMs

### 3. TimesFM
